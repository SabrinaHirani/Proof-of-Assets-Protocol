{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "12fb79a8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
                        "  warn(\n"
                    ]
                }
            ],
            "source": [
                "import random\n",
                "import math\n",
                "import numpy as np\n",
                "\n",
                "import torch\n",
                "from torch import nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "import pytorch_lightning as pl\n",
                "\n",
                "import json\n",
                "\n",
                "# check if notebook is in colab\n",
                "try:\n",
                "    # install ezkl\n",
                "    import google.colab\n",
                "    import subprocess\n",
                "    import sys\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ezkl\"])\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"onnx\"])\n",
                "\n",
                "# rely on local installation of ezkl if the notebook is not in colab\n",
                "except:\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "8638e94e",
            "metadata": {},
            "outputs": [],
            "source": [
                "class BaseDataModule(pl.LightningDataModule):\n",
                "    def __init__(self, batch_size=32, split=0.8, *args, **kwargs):\n",
                "        super().__init__()\n",
                "        self.contexts, self.questions, self.answers = self.get_dataset(*args, **kwargs)\n",
                "        self.split = int(len(self.contexts) * split)\n",
                "        self.batch_size = batch_size\n",
                "\n",
                "    def train_dataloader(self):\n",
                "        train_contexts = self.contexts[:self.split]\n",
                "        train_questions = self.questions[:self.split]\n",
                "        train_answers = self.answers[:self.split]\n",
                "        return torch.utils.data.DataLoader(list(zip(train_contexts, train_questions, train_answers)), batch_size=self.batch_size)\n",
                "\n",
                "    def val_dataloader(self):\n",
                "        val_contexts = self.contexts[self.split:]\n",
                "        val_questions = self.questions[self.split:]\n",
                "        val_answers = self.answers[self.split:]\n",
                "        return torch.utils.data.DataLoader(list(zip(val_contexts, val_questions, val_answers)), batch_size=self.batch_size)\n",
                "  \n",
                "class CustomDataModule(BaseDataModule):\n",
                "    def get_dataset(self):\n",
                "      context, question, answer = self.preprocess_custom_data(\"data.json\")\n",
                "      return context, question, answer\n",
                "    \n",
                "    def preprocess_custom_data(self, json_file):\n",
                "      with open(json_file, 'r', encoding='utf-8') as file:\n",
                "        data = json.load(file)\n",
                "    \n",
                "        contexts = []\n",
                "        questions = []\n",
                "        answers = []\n",
                "    \n",
                "        for entry in data:\n",
                "          context = entry['context']\n",
                "          question = entry['question']\n",
                "          answer_text = entry['answers']['text'][0]\n",
                "          answer_start = entry['answers']['answer_start'][0]\n",
                "        \n",
                "          contexts.append(context)\n",
                "          questions.append(question)\n",
                "          answers.append((answer_text, answer_start))\n",
                "    \n",
                "      return contexts, questions, answers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "323554ca",
            "metadata": {},
            "outputs": [],
            "source": [
                "def attention(queries, keys, values):\n",
                "  d = queries.shape[-1]\n",
                "  scores = torch.matmul(queries, keys.transpose(-2,-1))/math.sqrt(d)\n",
                "  attention_weights = F.softmax(scores, dim=-1)\n",
                "  return torch.matmul(attention_weights, values)\n",
                "\n",
                "class MultiHeadAttention(nn.Module):\n",
                "  def __init__(self, embed_dim, num_heads):\n",
                "    super(MultiHeadAttention, self).__init__()\n",
                "    self.embed_dim, self.num_heads = embed_dim, num_heads\n",
                "    assert embed_dim % num_heads == 0\n",
                "    self.projection_dim = embed_dim // num_heads\n",
                "    \n",
                "    self.W_q = nn.Linear(embed_dim, embed_dim)\n",
                "    self.W_k = nn.Linear(embed_dim, embed_dim)\n",
                "    self.W_v = nn.Linear(embed_dim, embed_dim)\n",
                "    self.W_o = nn.Linear(embed_dim, embed_dim)\n",
                "\n",
                "  def transpose(self, x):\n",
                "    x = x.reshape(x.shape[0], x.shape[1], self.num_heads, self.projection_dim)\n",
                "    return x.permute(0, 2, 1, 3)\n",
                "  \n",
                "  def transpose_output(self, x):\n",
                "    x = x.permute(0, 2, 1, 3)\n",
                "    return x.reshape(x.shape[0], x.shape[1], self.embed_dim)\n",
                "    \n",
                "  def forward(self, q, k, v):\n",
                "    q = self.transpose(self.W_q(q))\n",
                "    k = self.transpose(self.W_k(k))\n",
                "    v = self.transpose(self.W_v(v))\n",
                "    output = attention(q, k, v)\n",
                "    return self.W_o(self.transpose_output(output))\n",
                "  \n",
                "class TransformerBlock(nn.Module):\n",
                "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
                "    super(TransformerBlock, self).__init__()\n",
                "    self.att = MultiHeadAttention(embed_dim, num_heads)\n",
                "    self.ffn = nn.Sequential(\n",
                "      nn.Linear(embed_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, embed_dim)\n",
                "    )\n",
                "    self.layernorm1 = nn.LayerNorm(embed_dim)\n",
                "    self.layernorm2 = nn.LayerNorm(embed_dim)\n",
                "    self.dropout = nn.Dropout(rate)\n",
                "    \n",
                "  def forward(self, x):\n",
                "    x = self.layernorm1(x + self.dropout(self.att(x, x, x)))\n",
                "    x = self.layernorm2(x + self.dropout(self.ffn(x)))\n",
                "    return x\n",
                "  \n",
                "class TokenAndPositionEmbedding(nn.Module):\n",
                "  def __init__(self, maxlen, vocab_size, embed_dim):\n",
                "    super(TokenAndPositionEmbedding, self).__init__()\n",
                "    self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
                "    self.pos_emb = nn.Embedding(maxlen, embed_dim)\n",
                "    \n",
                "  def forward(self, x):\n",
                "    pos = torch.arange(0, x.size(1), dtype=torch.int32, device=x.device)\n",
                "    return self.token_emb(x) + self.pos_emb(pos).view(1, x.size(1), -1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "167e42e3",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "\n",
                "def compute_question_answering_loss(model_output, answer_spans):\n",
                "    # extract predicted start / end position logits\n",
                "    start_logits, end_logits = model_output\n",
                "    \n",
                "    # unpack answer spans\n",
                "    answer_text, answer_start = answer_spans\n",
                "\n",
                "    # convert answer spans to tensor\n",
                "    answer_start = torch.tensor(answer_start, dtype=torch.long).to(start_logits.device)\n",
                "    \n",
                "    # calculate cross-entropy loss for start / end positions\n",
                "    start_loss = F.cross_entropy(start_logits, answer_start)\n",
                "    \n",
                "    # sum / average loss as needed\n",
                "    total_loss = start_loss\n",
                "\n",
                "    return total_loss\n",
                "\n",
                "def evaluate_question_answering(predictions, references):\n",
                "    \"\"\"\n",
                "    Compute Exact Match (EM) and F1-score for question answering.\n",
                "\n",
                "    Args:\n",
                "        predictions (list): List of predicted answer strings.\n",
                "        references (list): List of reference answer strings.\n",
                "\n",
                "    Returns:\n",
                "        em (float): Exact Match score.\n",
                "        f1 (float): F1-score.\n",
                "    \"\"\"\n",
                "    assert len(predictions) == len(references), \"Number of predictions must match number of references.\"\n",
                "\n",
                "    em_count = 0\n",
                "    f1_total = 0\n",
                "\n",
                "    for pred, ref in zip(predictions, references):\n",
                "        # Tokenize predicted and reference answers\n",
                "        pred_tokens = pred.lower().split()\n",
                "        ref_tokens = ref.lower().split()\n",
                "\n",
                "        common_tokens = set(pred_tokens) & set(ref_tokens)\n",
                "        if not common_tokens:\n",
                "            # No common tokens, EM score is 0\n",
                "            em_count += 0\n",
                "            f1_total += 0\n",
                "        else:\n",
                "            # Compute F1-score\n",
                "            precision = len(common_tokens) / len(pred_tokens)\n",
                "            recall = len(common_tokens) / len(ref_tokens)\n",
                "            f1 = (2 * precision * recall) / (precision + recall)\n",
                "            f1_total += f1\n",
                "\n",
                "            # Exact Match (EM) is 1 if F1-score is 1, else 0\n",
                "            em_count += int(f1 == 1)\n",
                "\n",
                "    em = em_count / len(predictions)\n",
                "    f1 = f1_total / len(predictions)\n",
                "\n",
                "    return em, f1\n",
                "\n",
                "class LittleTransformer(pl.LightningModule):\n",
                "  def __init__(self, seq_len=6, max_value=10, layer_count=2, embed_dim=128, num_heads=4, ff_dim=32):\n",
                "    super().__init__()\n",
                "    self.max_value = max_value\n",
                "    self.model = nn.Sequential(\n",
                "      TokenAndPositionEmbedding(seq_len, max_value, embed_dim),\n",
                "      *[TransformerBlock(embed_dim, num_heads, ff_dim) for x in range(layer_count)],\n",
                "      nn.Linear(embed_dim, max_value),\n",
                "      nn.LogSoftmax(dim=-1))\n",
                "    \n",
                "  def forward(self, context, question):\n",
                "    input_embeddings = torch.cat((self.model.token_emb(context), self.model.token_emb(question)), dim=1)\n",
                "    return self.model(input_embeddings)\n",
                "  \n",
                "  def training_step(self, batch, batch_idx):\n",
                "    context, question, answer = batch\n",
                "    output = self.model(context, question)\n",
                "    loss = compute_question_answering_loss(output, answer)\n",
                "    self.log(\"train_loss\", loss)\n",
                "    return loss\n",
                "  \n",
                "  def validation_step(self, val_batch, batch_idx):\n",
                "    context, question, answer = val_batch\n",
                "    pred = self.model(context, question).argmax(dim=2)\n",
                "    evaluation_metrics = evaluate_question_answering(pred, answer)\n",
                "    self.log(\"val_metrics\", evaluation_metrics, prog_bar=True)\n",
                "  \n",
                "  def configure_optimizers(self):\n",
                "    if self.device.type == 'cuda':\n",
                "      import apex\n",
                "      return apex.optimizers.FusedAdam(self.parameters(), lr=3e-4)\n",
                "    else:\n",
                "      return torch.optim.Adam(self.parameters(), lr=3e-4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "a2f48c98",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "GPU available: False, used: False\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "HPU available: False, using: 0 HPUs\n",
                        "\n",
                        "  | Name  | Type       | Params\n",
                        "-------------------------------------\n",
                        "0 | model | Sequential | 153 K \n",
                        "-------------------------------------\n",
                        "153 K     Trainable params\n",
                        "0         Non-trainable params\n",
                        "153 K     Total params\n",
                        "0.613     Total estimated model params size (MB)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "09f4ec78bacb4ecd9f93cc0510cb47e8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "Sequential.forward() takes 2 positional arguments but 3 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32mc:\\Users\\sabri\\OneDrive\\Documents\\Projects\\Proof-of-Assets-Protocol\\zkml\\adapted_transformer_demo\\little_transformer.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sabri/OneDrive/Documents/Projects/Proof-of-Assets-Protocol/zkml/adapted_transformer_demo/little_transformer.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m LittleTransformer(seq_len\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sabri/OneDrive/Documents/Projects/Proof-of-Assets-Protocol/zkml/adapted_transformer_demo/little_transformer.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(enable_progress_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_epochs\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sabri/OneDrive/Documents/Projects/Proof-of-Assets-Protocol/zkml/adapted_transformer_demo/little_transformer.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data)\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[0;32m    544\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    546\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    547\u001b[0m )\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    575\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    576\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    577\u001b[0m     ckpt_path,\n\u001b[0;32m    578\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    580\u001b[0m )\n\u001b[1;32m--> 581\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    583\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    987\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 990\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    992\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    995\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1034\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m   1033\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1034\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[0;32m   1035\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1036\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1063\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1060\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1063\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1065\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1067\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[0;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[0;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[0;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    385\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_args)\n\u001b[0;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    395\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    396\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[0;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                        "\u001b[1;32mc:\\Users\\sabri\\OneDrive\\Documents\\Projects\\Proof-of-Assets-Protocol\\zkml\\adapted_transformer_demo\\little_transformer.ipynb Cell 5\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabri/OneDrive/Documents/Projects/Proof-of-Assets-Protocol/zkml/adapted_transformer_demo/little_transformer.ipynb#W4sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, val_batch, batch_idx):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabri/OneDrive/Documents/Projects/Proof-of-Assets-Protocol/zkml/adapted_transformer_demo/little_transformer.ipynb#W4sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m   context, question, answer \u001b[39m=\u001b[39m val_batch\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sabri/OneDrive/Documents/Projects/Proof-of-Assets-Protocol/zkml/adapted_transformer_demo/little_transformer.ipynb#W4sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m   pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(context, question)\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabri/OneDrive/Documents/Projects/Proof-of-Assets-Protocol/zkml/adapted_transformer_demo/little_transformer.ipynb#W4sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m   evaluation_metrics \u001b[39m=\u001b[39m evaluate_question_answering(pred, answer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabri/OneDrive/Documents/Projects/Proof-of-Assets-Protocol/zkml/adapted_transformer_demo/little_transformer.ipynb#W4sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mval_metrics\u001b[39m\u001b[39m\"\u001b[39m, evaluation_metrics, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                        "File \u001b[1;32mc:\\Users\\sabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
                        "\u001b[1;31mTypeError\u001b[0m: Sequential.forward() takes 2 positional arguments but 3 were given"
                    ]
                }
            ],
            "source": [
                "data = CustomDataModule(batch_size=64)\n",
                "model = LittleTransformer(seq_len=6)\n",
                "trainer = pl.Trainer(enable_progress_bar=True, max_epochs=0)\n",
                "trainer.fit(model, data)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fa7d277e",
            "metadata": {},
            "source": [
                "## EZKL "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6f339a28",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import os \n",
                "\n",
                "model_path = os.path.join('network.onnx')\n",
                "compiled_model_path = os.path.join('network.compiled')\n",
                "pk_path = os.path.join('test.pk')\n",
                "vk_path = os.path.join('test.vk')\n",
                "settings_path = os.path.join('settings.json')\n",
                "srs_path = os.path.join('kzg.srs')\n",
                "witness_path = os.path.join('witness.json')\n",
                "data_path = os.path.join('input.json')\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "27ce542b",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import json \n",
                "# After training, export to onnx (network.onnx) and create a data file (input.json)\n",
                "x = torch.ones([1, 6], dtype=torch.long)\n",
                "x = x.reshape([1, 6])\n",
                "\n",
                "print(x)\n",
                "\n",
                "# Flips the neural net into inference mode\n",
                "model.eval()\n",
                "model.to('cpu')\n",
                "\n",
                "    # Export the model\n",
                "torch.onnx.export(model,               # model being run\n",
                "                      x,                   # model input (or a tuple for multiple inputs)\n",
                "                      model_path,            # where to save the model (can be a file or file-like object)\n",
                "                      export_params=True,        # store the trained parameter weights inside the model file\n",
                "                      opset_version=10,          # the ONNX version to export the model to\n",
                "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
                "                      input_names = ['input'],   # the model's input names\n",
                "                      output_names = ['output'], # the model's output names\n",
                "                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
                "                                    'output' : {0 : 'batch_size'}})\n",
                "\n",
                "data_array = ((x).detach().numpy()).reshape([-1]).tolist()\n",
                "\n",
                "data_json = dict(input_data = [data_array])\n",
                "\n",
                "print(data_json)\n",
                "\n",
                "    # Serialize data into file:\n",
                "json.dump( data_json, open(data_path, 'w' ))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "36ddc6f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import ezkl \n",
                "\n",
                "!RUST_LOG=trace\n",
                "# TODO: Dictionary outputs\n",
                "res = ezkl.gen_settings(model_path, settings_path)\n",
                "assert res == True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2fe6d972",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "res = await ezkl.calibrate_settings(data_path, model_path, settings_path, \"resources\")\n",
                "assert res == True\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0990f5a8",
            "metadata": {},
            "outputs": [],
            "source": [
                "res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
                "assert res == True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1b80dc01",
            "metadata": {},
            "outputs": [],
            "source": [
                "# srs path\n",
                "res = ezkl.get_srs(srs_path, settings_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "54cbde29",
            "metadata": {},
            "outputs": [],
            "source": [
                "# now generate the witness file \n",
                "witness_path = \"gan_witness.json\"\n",
                "\n",
                "res = ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
                "assert os.path.isfile(witness_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28760638",
            "metadata": {},
            "outputs": [],
            "source": [
                "res = ezkl.mock(witness_path, compiled_model_path)\n",
                "assert res == True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5e595112",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# HERE WE SETUP THE CIRCUIT PARAMS\n",
                "# WE GOT KEYS\n",
                "# WE GOT CIRCUIT PARAMETERS\n",
                "# EVERYTHING ANYONE HAS EVER NEEDED FOR ZK\n",
                "\n",
                "res = ezkl.setup(\n",
                "        compiled_model_path,\n",
                "        vk_path,\n",
                "        pk_path,\n",
                "        srs_path,\n",
                "    )\n",
                "\n",
                "assert res == True\n",
                "assert os.path.isfile(vk_path)\n",
                "assert os.path.isfile(pk_path)\n",
                "assert os.path.isfile(settings_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d37adaef",
            "metadata": {},
            "outputs": [],
            "source": [
                "# GENERATE A PROOF\n",
                "\n",
                "\n",
                "proof_path = os.path.join('test.pf')\n",
                "\n",
                "proof = ezkl.prove(\n",
                "        witness_path,\n",
                "        compiled_model_path,\n",
                "        pk_path,\n",
                "        proof_path,\n",
                "        srs_path,\n",
                "        \"single\",\n",
                "    )\n",
                "\n",
                "print(proof)\n",
                "assert os.path.isfile(proof_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5b58acd5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# VERIFY IT\n",
                "res = ezkl.verify(\n",
                "        proof_path,\n",
                "        settings_path,\n",
                "        vk_path,\n",
                "        srs_path,\n",
                "    )\n",
                "\n",
                "assert res == True\n",
                "print(\"verified\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7b2232c7",
            "metadata": {},
            "outputs": [],
            "source": [
                "sol_code_path = os.path.join('Verifier.sol')\n",
                "abi_path = os.path.join('Verifier.abi')\n",
                "\n",
                "res = ezkl.create_evm_verifier(\n",
                "        vk_path,\n",
                "        srs_path,\n",
                "        settings_path,\n",
                "        sol_code_path,\n",
                "        abi_path\n",
                "    )\n",
                "\n",
                "assert res == True\n",
                "assert os.path.isfile(sol_code_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "64bd70f3",
            "metadata": {},
            "outputs": [],
            "source": [
                "onchain_input_array = []\n",
                "\n",
                "# using a loop\n",
                "# avoiding printing last comma\n",
                "formatted_output = \"[\"\n",
                "for i, value in enumerate(proof[\"instances\"]):\n",
                "    for j, field_element in enumerate(value):\n",
                "        onchain_input_array.append(ezkl.vecu64_to_felt(field_element))\n",
                "        formatted_output += str(onchain_input_array[-1])\n",
                "        if j != len(value) - 1:\n",
                "            formatted_output += \", \"\n",
                "    formatted_output += \"]\"\n",
                "\n",
                "# This will be the values you use onchain\n",
                "# copy them over to remix and see if they verify\n",
                "# What happens when you change a value?\n",
                "print(\"pubInputs: \", formatted_output)\n",
                "print(\"proof: \", \"0x\" + proof[\"proof\"])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
